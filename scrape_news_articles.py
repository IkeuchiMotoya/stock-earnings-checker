import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
import pandas as pd
import os

# ================================
# ğŸ”§ è¨­å®šï¼ˆå–å¾—æœŸé–“ã®å®šæ•°ï¼‰
# ================================
DAYS_BACK = 7  # â† ã“ã“ã‚’å¤‰æ›´ã™ã‚Œã°å¯¾è±¡æœŸé–“ã‚’ç°¡å˜ã«å¤‰æ›´å¯èƒ½
JST = timezone(timedelta(hours=9))
END_DATE = datetime.now(JST)
START_DATE = END_DATE - timedelta(days=DAYS_BACK)

# ================================
# ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–¢æ•°ï¼ˆæœŸé–“æŒ‡å®šï¼‰
# ================================
def fetch_news_in_range(stock_code, start_date, end_date):
    url = f'https://kabutan.jp/stock/news?code={stock_code}'
    resp = requests.get(url)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, 'html.parser')

    news_rows = soup.select('table.s_news_list tr')
    collected_news = []

    for row in news_rows:
        time_td = row.find('td', class_='news_time')
        if not time_td:
            continue
        time_text = time_td.text.strip()
        try:
            news_date = datetime.strptime(time_text[:8], '%y/%m/%d').replace(tzinfo=JST)
        except ValueError:
            continue
        if start_date <= news_date <= end_date:
            title_td = row.find_all('td')[-1]
            a_tag = title_td.find('a')
            if a_tag:
                title = a_tag.text.strip()
                href = a_tag['href']
                link = href if href.startswith('http') else 'https://kabutan.jp' + href
                collected_news.append({
                    'éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰': stock_code,
                    'éŠ˜æŸ„å': "",
                    'æ™‚é–“': time_text,
                    'ã‚¿ã‚¤ãƒˆãƒ«': title,
                    'URL': link
                })

    return collected_news

# ================================
# ğŸ’¾ Excelä¿å­˜é–¢æ•°
# ================================
def save_to_excel(news_data, filepath):
    df = pd.DataFrame(news_data)
    df.to_excel(filepath, index=False)
    os.startfile(filepath)

# ================================
# ğŸ” ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ================================
if __name__ == '__main__':
    csv_path = r'C:\Users\pumpk\OneDrive\ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—\æ ªå¼\csv\ä¿æœ‰éŠ˜æŸ„\ä¿æœ‰éŠ˜æŸ„.csv'
    stock_df = pd.read_csv(csv_path, dtype=str)
    stock_list = stock_df.to_dict('records')

    all_news = []

    for stock in stock_list:
        code = str(stock['éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰'])
        name = stock['éŠ˜æŸ„å']
        news = fetch_news_in_range(code, START_DATE, END_DATE)
        for n in news:
            n['éŠ˜æŸ„å'] = name
        all_news.extend(news)

    if all_news:
        output_path = r'C:\Users\pumpk\OneDrive\ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—\æ ªå¼\kabutan_news_æœŸé–“æŒ‡å®š.xlsx'
        save_to_excel(all_news, output_path)
        print(f"{len(all_news)} ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’Excelã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    else:
        print("æŒ‡å®šæœŸé–“ã«è©²å½“ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
